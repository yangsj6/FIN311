{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L15: Text Analysis (Part I)\n",
    "[1. Regular Expressions](#1.-Regular-Expressions)\\\n",
    "[2. Dictionary-Based Textual Analysis](#2.-Dictionary-Based-Textual-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python's regular expressions module `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Patterns in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to match a basic regex `r\"OI\"`To perform case-insensitive Regex matching, we can either specify IGNORECASE flag in a re function or convert the input string to lower case using string’s lower() method: in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of re. search :\n",
      "<re.Match object; span=(0, 2), match='OI'>\n",
      "\n",
      "Result of re. findall :\n",
      "['OI', 'OI']\n",
      "\n",
      "Result of re. split :\n",
      "['OI for FY 2019 was 12.4 billion', ' up more than eight percent from OI in FY 2018.']\n",
      "\n",
      "Result of re.sub :\n",
      "Operating Income for FY 2019 was 12.4 billion, up more than eight percent from Operating Income in FY 2018.\n"
     ]
    }
   ],
   "source": [
    "# loads Python's regular expressions module\n",
    "import re\n",
    "\n",
    "text = \"OI for FY 2019 was 12.4 billion, up more than eight percent from OI in FY 2018.\"\n",
    "\n",
    "# returns a Match object of the first match, if it exists\n",
    "x1 = re.search(r\"OI\", text)\n",
    "\n",
    "# finds all matches of \"OI\"\n",
    "x2 = re.findall(r\"OI\", text)\n",
    "\n",
    "# splits text at \",\"\n",
    "x3 = re.split(r\",\", text)\n",
    "\n",
    "# replaces \"OI\" with \" Operating Income \"\n",
    "x4 = re.sub(r\"OI\", \"Operating Income\", text)\n",
    "\n",
    "print(f'Result of re. search :\\n{x1}\\n')\n",
    "print(f'Result of re. findall :\\n{x2}\\n')\n",
    "print(f'Result of re. split :\\n{x3}\\n')\n",
    "print(f'Result of re.sub :\\n{x4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform case-insensitive Regex matching, we can either specify IGNORECASE flag in a `re` function or convert the input string to lower case using string’s `lower()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MD&a', 'md&A']\n",
      "['md&a', 'md&a']\n"
     ]
    }
   ],
   "source": [
    "x1 = re.findall(r'MD&A', \" This year's MD&a Section is located ... Please refer to our md&A section on page ... \", re.IGNORECASE)\n",
    "x2 = re.findall(r'md&a', \" This year's MD&a Section is located ... Please refer to our md&A section on page ... \".lower())\n",
    "                   \n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Sets in Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7', '0', '2', '0', '1', '9']\n",
      "['7', '0', '%', '2', '0', '1', '9']\n",
      "['70%']\n"
     ]
    }
   ],
   "source": [
    "text = \"This project has increased our revenues by more than 70% in FY 2019.\"\n",
    "\n",
    "# returns all single digit matches\n",
    "x1 = re . findall(r'[0-9]', text)\n",
    "\n",
    "# returns all non - word characters, also excludes spaces, periods, and commas\n",
    "x2 = re.findall(r'[^a-zA-Z \\.,]', text)\n",
    "\n",
    "# returns all two - digit numbers followed by \"%\"\n",
    "x3 = re.findall(r'\\d\\d%', text)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups in Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we expect this trend to continue\n"
     ]
    }
   ],
   "source": [
    "regex = r\"\\b(?P<word>\\w+)\\s(?P=word)\\b\"\n",
    "text = \"we we expect this this trend to continue\"\n",
    "\n",
    "# prints an output where all double words are replaced with a single word\n",
    "print(re.sub(regex,r\"\\g<word>\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month:  09\n",
      "Day:  14\n",
      "Year:  2020\n"
     ]
    }
   ],
   "source": [
    "date = \"09/14/2020\"\n",
    "\n",
    "# specifies three named groups, namely 'Month', 'Day', and 'Year'\n",
    "regex = r\"(?P<Month>\\d{1,2})/(?P<Day>\\d{1,2})/(?P<Year>\\d{2,4})\"\n",
    "\n",
    "# identifies regex matches in date\n",
    "date_matches = re.search(regex, date)\n",
    "# prints matches for each of the named groups\n",
    "print(\"Month: \", date_matches.group('Month'))\n",
    "print(\"Day: \", date_matches.group('Day'))\n",
    "print(\"Year: \", date_matches.group('Year'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Character Sets**\n",
    "\n",
    "Assume we want to identify all numbers in text followed by % or the word percent. To do so, we create a character set of digits (allowing for period ‘.’ in numbers); we also specify the “% vs. percent” option using the regex “or” symbol, |."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['70%', '9%', '12%', '12.5 percent']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"This project has resulted in over 70% of our 2019 revenues to date. \n",
    "As a result, our operating income increased by 9%, while our operating expenses \n",
    "increased by 12%. We had a 12.5 percent increase in regional sales.\"\"\"\n",
    "\n",
    "# recall that ?: after the left parenthesis specifies a non-capturing group\n",
    "x = re.findall (r'[\\d\\.]+(?:\\%|\\s\\bpercent\\b)', text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Character Sets, Quantifiers, Groups, Lookbehinds**\n",
    "\n",
    "Assume we want to identify basic company information – CIK number, company name, filing date, and SIC industry code – from the company filing’s SEC header. To do so, we need to create separate regular expressions for all items that we want to capture.\\\n",
    "The following SEC filing is from https://www.sec.gov/Archives/edgar/data/80424/000008042418000100/0000080424-18-000100.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000080424']\n",
      "['PROCTER & GAMBLE Co']\n",
      "['20181019']\n",
      "['2840']\n"
     ]
    }
   ],
   "source": [
    "# an example of a standard input header used in SEC filings\n",
    "header = \"\"\"<SEC-DOCUMENT>0000080424-18-000100.txt : 20181019\n",
    "<SEC-HEADER>0000080424-18-000100.hdr.sgml : 20181019\n",
    "<ACCEPTANCE-DATETIME>20181019161731\n",
    "ACCESSION NUMBER:\t\t0000080424-18-000100\n",
    "CONFORMED SUBMISSION TYPE:\t10-Q\n",
    "PUBLIC DOCUMENT COUNT:\t\t68\n",
    "CONFORMED PERIOD OF REPORT:\t20180930\n",
    "FILED AS OF DATE:\t\t20181019\n",
    "DATE AS OF CHANGE:\t\t20181019\n",
    "\n",
    "FILER:\n",
    "\n",
    "\tCOMPANY DATA:\t\n",
    "\t\tCOMPANY CONFORMED NAME:\t\t\tPROCTER & GAMBLE Co\n",
    "\t\tCENTRAL INDEX KEY:\t\t\t0000080424\n",
    "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tSOAP, DETERGENT, CLEANING PREPARATIONS, PERFUMES, COSMETICS [2840]\n",
    "\t\tIRS NUMBER:\t\t\t\t310411980\n",
    "\t\tSTATE OF INCORPORATION:\t\t\tOH\n",
    "\t\tFISCAL YEAR END:\t\t\t0630\n",
    "[...]\n",
    "</SEC-HEADER>\"\"\"\n",
    "\n",
    "# CIK is the 10 - digit number, so we use the quantifier\n",
    "# {10} to consider only 10 - digit numbers in the match\n",
    "# this Regex specifies text \" CENTRAL INDEX KEY:\",\n",
    "# followed by space \\s(matched zero or many times\n",
    "# as indicated by *), followed by a group capturing\n",
    "# 10 - digit numbers\n",
    "# Also, note that re. findall with a group Regex only\n",
    "# returns the group match, and not the full match\n",
    "cik = re.findall(r\"CENTRAL INDEX KEY:\\s*(\\d{10})\", header)\n",
    "\n",
    "# This Regex specifies text \" COMPANY CONFORMED NAME:\",\n",
    "# followed by space \\s(matched zero or many times),\n",
    "# followed by a group capturing any character one or\n",
    "# many times\n",
    "# flag MULTILINE makes ^ and $ characters capture\n",
    "# beginning and end positions of text lines instead\n",
    "# of only text files\n",
    "company_name = re.findall(r\"COMPANY CONFORMED NAME:\\s*(.+)$\", header, re.MULTILINE)\n",
    "\n",
    "# This Regex specifies text \"FILED AS OF DATE:\",\n",
    "# followed by space \\s(matched zero or many times),\n",
    "# followed by a group capturing 8 - digit numbers as\n",
    "# all dates in the SEC headers are in the YYYYMMDD\n",
    "# format\n",
    "filing_date = re.findall(r'FILED AS OF DATE:\\s*(\\d{8})', header)\n",
    "\n",
    "# This Regex uses a positive lookbehind to check if a\n",
    "# 4 - digit number is preceded by text \" STANDARD\n",
    "# INDUSTRIAL CLASSIFICATION:\"\n",
    "sic = re.findall(r'(?<=STANDARD INDUSTRIAL CLASSIFICATION:).+(\\d{4})', header)\n",
    "\n",
    "print(cik)\n",
    "print(company_name)\n",
    "print(filing_date)\n",
    "print(sic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Word Boundaries and Quantifiers**\n",
    "\n",
    "Assume we want to identify all word matches that start with “risk”, but can have different endings (e.g., risks, risky, risking, etc.). We also want to calculate the percentage of such risk words relative to all words in text. To do so, we use regex word boundaries to perform single word matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['risk', 'risks', 'risks', 'Risk', 'risks', 'risky', 'riskiness', 'risks']\n",
      "11.428571428571429\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"An investment in our common stock involves a\n",
    "high degree of risk. You should carefully consider\n",
    "the risks summarized below. The risks are discussed\n",
    "more fully in the Risk Factors section of this\n",
    "prospectus immediately following this prospectus\n",
    "summary. These risks include, but are not limited\n",
    "to, the following [...] These operations are risky\n",
    "[...] Macroeconomic fluctuations increase the\n",
    "riskiness of our operations. As indicated in\n",
    "Section 2.1, our company's long-term risks include\n",
    "[...] \"\"\"\n",
    "\n",
    "# this Regex matches a word boundary followed by a\n",
    "# text string 'risk ', followed by an alphanumeric\n",
    "# character(repeated zero or many times), followed\n",
    "# by a word boundary ; re. IGNORECASE specifies a\n",
    "# case - insensitive matching\n",
    "risk_words = re.findall(r\"\\brisk\\w*\\b\", text, re.IGNORECASE)\n",
    "\n",
    "# matches all single words(allowing for '-'\n",
    "# between two words and apostrophe)in text\n",
    "all_words = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", text)\n",
    "\n",
    "# function len () here returns the number of words\n",
    "# that start with string 'risk ', i.e., the number\n",
    "# of matches in risk_words list\n",
    "risk_words_freq = len(risk_words)\n",
    "\n",
    "# the number of all words in text\n",
    "all_words_freq = len(all_words)\n",
    "\n",
    "# percentage of risk - related words in text\n",
    "text_riskiness = 100 * risk_words_freq / all_words_freq\n",
    "\n",
    "print(risk_words)\n",
    "print(text_riskiness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9Tnd8cwpKzz"
   },
   "source": [
    "# 2. Dictionary-Based Textual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Words and Sentences in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'invested', 'in', 'six', 'areas', 'of', 'the', 'business', 'that', 'account', 'for', 'nearly', 'of', 'total', \"Macy's\", 'sales', 'Dresses', 'fine', 'jewelry', 'big', 'ticket', \"men's\", 'tailored', \"women's\", 'shoes', 'and', 'beauty', 'these', 'investments', 'were', 'aimed', 'at', 'driving', 'growth', 'through', 'great', 'products', 'top-performing', 'colleagues', 'improved', 'environment', 'and', 'enhanced', 'marketing', 'All', 'six', 'areas', 'continued', 'to', 'outperform', 'the', 'balance', 'of', 'the', 'business', 'on', 'market', 'share', 'return', 'on', 'investment', 'and', 'profitability', 'And', 'we', 'capture', 'approximately', 'of', 'the', 'market', 'in', 'these', 'categories']\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"We invested in six areas of the business that\n",
    "account for nearly 40% of total Macy's sales.\n",
    "Dresses, fine jewelry, big ticket, men's tailored,\n",
    "women's shoes and beauty, these investments were\n",
    "aimed at driving growth through great products, \n",
    "top-performing colleagues, improved environment and\n",
    "enhanced marketing. All six areas continued to\n",
    "outperform the balance of the business on market\n",
    "share, return on investment and profitability. And\n",
    "we capture approximately 9% of the market in these\n",
    "categories.\"\"\"\n",
    "\n",
    "x = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", text)\n",
    "# Regex \"\\b[a-zA-Z\\'\\-]+\\b\" searches for all words in\n",
    "# text, allowing apostrophes and hyphens in words,\n",
    "# e.g., company's, state-of-the-art\n",
    "\n",
    "print(x)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 We invested in six areas of the business that\n",
      "account for nearly 40% of total Macy's sales.\n",
      "2 Dresses, fine jewelry, big ticket, men's tailored,\n",
      "women's shoes and beauty, these investments were\n",
      "aimed at driving growth through great products, \n",
      "top-performing colleagues, improved environment and\n",
      "enhanced marketing.\n",
      "3 All six areas continued to\n",
      "outperform the balance of the business on market\n",
      "share, return on investment and profitability.\n",
      "4 And\n",
      "we capture approximately 9% of the market in these\n",
      "categories.\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern that identifies a sentence\n",
    "# re. compile compiles a regular expression pattern\n",
    "# into a regular expression object in Python\n",
    "sentence_regex=re.compile(r\"\\b[A-Z](?:[^\\.!?]|\\.\\d)*[\\.!?]\")\n",
    "\n",
    "def identify_sentences(input_text: str):\n",
    "    # finds all matches of sentence_regex in input_text\n",
    "    sentences = re.findall(sentence_regex, input_text)\n",
    "    return sentences\n",
    "\n",
    "sentences = identify_sentences(text)\n",
    "\n",
    "# enumerate is a Python function that when applied to\n",
    "# a list , returns list elements along with their\n",
    "# indexes(counter); 1 indicates that the counter\n",
    "# should start from 1 instead of default 0\n",
    "for counter, sentence in enumerate(sentences, 1):\n",
    "    print(counter, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuamdOs5pKz0"
   },
   "source": [
    "### spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `spacy` and its English (or other language) model: \\\n",
    "https://spacy.io/usage \\\n",
    "https://spacy.io/models/en\n",
    "* pip install -U pip setuptools wheel\n",
    "* pip install -U spacy\n",
    "* python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `spacy` to identifying words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Jv6KYhPzpKz0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'invested', 'in', 'six', 'areas', 'of', 'the', 'business', 'that', '\\n', 'account', 'for', 'nearly', '40', '%', 'of', 'total', 'Macy', \"'s\", 'sales', '.', '\\n', 'Dresses', ',', 'fine', 'jewelry', ',', 'big', 'ticket', ',', 'men', \"'s\", 'tailored', ',', '\\n', 'women', \"'s\", 'shoes', 'and', 'beauty', ',', 'these', 'investments', 'were', '\\n', 'aimed', 'at', 'driving', 'growth', 'through', 'great', 'products', ',', '\\n', 'top', '-', 'performing', 'colleagues', ',', 'improved', 'environment', 'and', '\\n', 'enhanced', 'marketing', '.', 'All', 'six', 'areas', 'continued', 'to', '\\n', 'outperform', 'the', 'balance', 'of', 'the', 'business', 'on', 'market', '\\n', 'share', ',', 'return', 'on', 'investment', 'and', 'profitability', '.', 'And', '\\n', 'we', 'capture', 'approximately', '9', '%', 'of', 'the', 'market', 'in', 'these', '\\n', 'categories', '.']\n",
      "1 We invested in six areas of the business that\n",
      "account for nearly 40% of total Macy's sales.\n",
      "\n",
      "2 Dresses, fine jewelry, big ticket, men's tailored,\n",
      "women's shoes and beauty, these investments were\n",
      "aimed at driving growth through great products, \n",
      "top-performing colleagues, improved environment and\n",
      "enhanced marketing.\n",
      "3 All six areas continued to\n",
      "outperform the balance of the business on market\n",
      "share, return on investment and profitability.\n",
      "4 And\n",
      "we capture approximately 9% of the market in these\n",
      "categories.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the English language model in spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# create an \"nlp\" object that parses a textual document\n",
    "a_text = nlp(text)\n",
    "\n",
    "# create a list of word tokens; note, this list will\n",
    "# include punctuation marks and other symbols\n",
    "token_list = []\n",
    "for token in a_text:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)\n",
    "\n",
    "sentences = list(a_text.sents)\n",
    "\n",
    "# print all sentences\n",
    "for counter, sentence in enumerate(sentences, 1) :\n",
    "    print(counter, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import NLTK’s stemming and lemmatization modules and then apply stem and lemmatize commands to the words of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for 'increasing' is increas\n",
      "Stemming for 'increases' is increas\n",
      "Stemming for 'increased' is increas\n",
      "Lemmatization for 'increasing' is increase\n",
      "Lemmatization for 'increases' is increase\n",
      "Lemmatization for 'increased' is increase\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import Porter stemmer Module\n",
    "from nltk.stem import PorterStemmer\n",
    "# import WordNet lemmatization Module\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# object for Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# object for WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Then , performing stemming on single words is as simple as:\n",
    "print(f\"Stemming for 'increasing' is {stemmer.stem('increasing')}\")\n",
    "print(f\"Stemming for 'increases' is {stemmer.stem('increases')}\")\n",
    "print(f\"Stemming for 'increased' is {stemmer.stem('increased')}\")\n",
    "\n",
    "# To improve the accuracy of lemmatization , we need to\n",
    "# provide each word 's part of the speech (POS) specifying POS as verb \"v\"\n",
    "print(f\"Lemmatization for 'increasing' is {lemmatizer.lemmatize('increasing', pos='v')}\")\n",
    "print(f\"Lemmatization for 'increases' is {lemmatizer.lemmatize('increases', pos='v')}\")\n",
    "print(f\"Lemmatization for 'increased' is {lemmatizer.lemmatize('increased', pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing lemmatization or stemming on a sentence level requires more work as we need to split sentences into single words and identify each word’s part of the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we deliv adjust earn per share of $ 2.12 . for the year , compar sale were down 0.7 % on an own plu licens basi , and we deliv adjust earn per share of $ 2.91 .\n",
      "We deliver adjusted earnings per share of $ 2.12 . For the year , comparable sale be down 0.7 % on an owned plus licensed basis , and we deliver adjusted earnings per share of $ 2.91 .\n"
     ]
    }
   ],
   "source": [
    "# WordNet is just another NLTK corpus reader\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import NLTK tokenizer and (part of speech) POS tagger\n",
    "from nltk import word_tokenize, pos_tag\n",
    "# import Porter stemmer class\n",
    "from nltk.stem import PorterStemmer\n",
    "# import WordNet lemmatizer class\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# default dictionary is similar to Python 's regular\n",
    "# dictionary, but allows the dictionary to return a\n",
    "# default value if a requested key does not exist in\n",
    "# the dictionary from collections import defaultdict\n",
    "\n",
    "# object for Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# object for WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create a dictionary where single-letter keys are\n",
    "# mapped to part of speech (noun, adjective, etc.)\n",
    "# WordNet identifiers; by default, if a key does not\n",
    "# exists the dictionary , return noun(wordnet.NOUN)\n",
    "tag_map = nltk.defaultdict(lambda: wordnet.NOUN)\n",
    "# add key 'J' to the dictionary indicating adjective\n",
    "tag_map ['J'] = wordnet.ADJ\n",
    "# add key 'V' to the dictionary indicating verb\n",
    "tag_map ['V'] = wordnet.VERB\n",
    "# add key 'R' to the dictionary indicating adverb\n",
    "tag_map ['R'] = wordnet.ADV\n",
    "\n",
    "text = \"\"\"We delivered adjusted earnings per share of $2.12.\n",
    "For the year, comparable sales were down 0.7%\n",
    "on an owned plus licensed basis, and we delivered\n",
    "adjusted earnings per share of $2.91.\"\"\"\n",
    "\n",
    "# function that stems text\n",
    "def stem_text(text:str):\n",
    "    # split text into(word)tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_text = []\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token)\n",
    "        stemmed_text.append(stem)\n",
    "    # concatenate stemmed tokens elements with\n",
    "    # space (\" \") in-between\n",
    "    return \" \".join(stemmed_text)\n",
    "\n",
    "# function that to lemmatizes text\n",
    "def lemmatize_text(text:str) :\n",
    "    # splits text into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # lemmatize word tokens , tag [0] returns POS\n",
    "        # letter identifier\n",
    "        lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "        lemmatized_text.append(lemma)\n",
    "    # concatenate lemmatized tokens elements with\n",
    "    # space in - between\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "# print stemmed version of text\n",
    "print(stem_text(text))\n",
    "# print lemmatized version of text\n",
    "print(lemmatize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tone Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by uploading dictionaries of words that we are interested in counting. \\\n",
    "We recommend having these dictionary files in plain text (.txt) format, either tab delimited or comma separated, with every dictionary word / phrase in a separate line. \\\n",
    "In the example below, both “positive.txt” and “negative.txt” dictionary files contain base-form words as well as inflected words (e.g., increase, increases, increasing, increased), so we do not need to perform word stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[re.compile('\\\\bable\\\\b'), re.compile('\\\\babundance\\\\b'), re.compile('\\\\babundant\\\\b')]\n",
      "[re.compile('\\\\babandon\\\\b'), re.compile('\\\\babandoned\\\\b'), re.compile('\\\\babandoning\\\\b')]\n"
     ]
    }
   ],
   "source": [
    "# Let us start with a simple tone analysis, where each\n",
    "# word is equally-weighted and we do not account for\n",
    "# negators.\n",
    "\n",
    "# First , we need to specify the locations of\n",
    "# our dictionary files.\n",
    "# file path(location)to a text file with positive\n",
    "# words ; every word is in a separate line in the file\n",
    "positive_words_dict = r\"./positive.txt\"\n",
    "# file path to a text file with negative words\n",
    "negative_words_dict = r\"./negative.txt\"\n",
    "\n",
    "# To be able to match all positive and negative words\n",
    "# from the dictionaries , we need to create a list of\n",
    "# regular expressions corresponding to these words\n",
    "\n",
    "# The following function reads all dictionary terms\n",
    "# to a Python list , and converts the terms regular\n",
    "# expressions\n",
    "\n",
    "def create_dict_regex_list(dict_file:str):\n",
    "    \"\"\"Creates a list of regex expressions of\n",
    "    dictionary terms.\"\"\"\n",
    "    # opens the specified dict_file in \"r\"(read)mode\n",
    "    with open(dict_file ,\"r\") as file :\n",
    "        # reads the content of the file\n",
    "        # line -by - line and creates a list of\n",
    "        # dictionary phrases\n",
    "        dict_terms = file.read().splitlines()\n",
    "    # re. compile(pattern)in Python compiles a regular\n",
    "    # expression pattern , which can be used for\n",
    "    # matching using its re.search , re. findall , etc.\n",
    "    # by adding \"\\b\" (i.e. , word boundary)on each\n",
    "    # side of a dictionary term in Regex , we force\n",
    "    # an exact match that dictionary term\n",
    "    dict_terms_regex = [re.compile(r'\\b' + term + r'\\b') for term in dict_terms]\n",
    "    # specifies the output of the function - in our\n",
    "    # case , a list of Regex expressions that\n",
    "    # correspond to the input dictionary file\n",
    "    return dict_terms_regex\n",
    "\n",
    "# Now we can apply our function to create Regex lists\n",
    "# for positive and negative dictionary terms\n",
    "positive_dict_regex = create_dict_regex_list(positive_words_dict)\n",
    "negative_dict_regex = create_dict_regex_list(negative_words_dict)\n",
    "\n",
    "# print the first three entries of each Regex dictionary\n",
    "print(positive_dict_regex[0:3])\n",
    "print(negative_dict_regex[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to write a function that will count positive, negative, and all words in a given text, so we can calculate document Tone as follows:\\\n",
    "$Tone(\\%)=100×\\dfrac{(PositiveWordCount − NegativeWordCount)}{TotalWordCount}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 7, 0, 6.140350877192983)\n"
     ]
    }
   ],
   "source": [
    "def get_tone(input_text:str):\n",
    "    \"\"\" Counts All and Specific Words in Text \"\"\"\n",
    "\n",
    "    ### Positive Words ###\n",
    "\n",
    "    # finds all regex matches and returns them as a\n",
    "    # list of lists so, the output of this search\n",
    "    # will be of the following format :\n",
    "    # [['able'] , [] , ['abundant','abundant'] , [] , ... ]\n",
    "\n",
    "    positive_words_matches = [re.findall(regex, input_text) for regex in positive_dict_regex]\n",
    "\n",
    "    # len() measures the length of each list match\n",
    "    # so , the output of this list transformation\n",
    "    # will be of the following format: [1, 0 , 2, 0,...]\n",
    "    positive_words_counts = [len(match) for match in positive_words_matches]\n",
    "    positive_words_sum = sum(positive_words_counts)\n",
    "\n",
    "    ### Negative Words ###\n",
    "\n",
    "    # in similar manner , we can get word counts for\n",
    "    # negative words finds all matches of negative words'\n",
    "    # regular expressions\n",
    "    negative_words_matches = [re.findall(regex, input_text) for regex in negative_dict_regex]\n",
    "\n",
    "    # calculates the number of matches for each\n",
    "    # dictionary term regex\n",
    "    negative_words_counts = [len(match) for match in negative_words_matches]\n",
    "    negative_words_sum = sum(negative_words_counts)\n",
    "\n",
    "    ### Total Words ###\n",
    "\n",
    "    # searches for all words in text, allowing\n",
    "    # apostrophes and hyphens in words, e.g.,\n",
    "    # \"company's\" , \"state-of-the-art\"\n",
    "    total_words = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", input_text)\n",
    "\n",
    "    # calculates the number of all words in text\n",
    "    total_words_count = len(total_words)\n",
    "\n",
    "    # Finally , we can calculate Tone\n",
    "    #(expressed in % terms)as:\n",
    "    tone = 100 * (positive_words_sum-negative_words_sum) / total_words_count\n",
    "    return (total_words_count, positive_words_sum, negative_words_sum, tone)\n",
    "\n",
    "# Applying our count_words function to an input text :\n",
    "counts = get_tone(\"\"\"At FedEx Ground , we have the market\n",
    "leading e-commerce portfolio. We continue to see\n",
    "strong demand across all customer segments with our\n",
    "new seven-day service . We will increase our speed\n",
    "advantage during the New Year. Our Sunday roll-out\n",
    "will speed up some lanes by one and two full\n",
    "transit days. This will increase our advantage\n",
    "significantly. And as you know, we are already\n",
    "faster by at least one day when compared to UPS's\n",
    "ground service in 25% of lanes. It is also really\n",
    "important to note our speed advantage and seven-day\n",
    "service is also very valuable for the premium B2B\n",
    "sectors, including healthcare and perishables\n",
    "shippers. Now, turning to Q2, I'm not pleased with\n",
    "our financial results.\"\"\")\n",
    "\n",
    "# output the results as (Total Word Count,\n",
    "# Number of Positive Words , Number of Negative Words ,\n",
    "# Tone)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of negators we might want to consider in our regular expressions: \\\n",
    "**not, never, no, none, nobody, nothing, don’t, doesn’t, won’t, shan’t, didn’t, shouldn’t, wouldn’t, couldn’t, can’t, cannot, neither, nor** \\\n",
    "To account for negators in our previous code, we need to rewrite our regular expressions for word counts as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile(\"(not|never|no|none|nobody|nothing|don\\\\'t\\\\\\n|doesn\\\\'t|won\\\\'t|shan\\\\'t|didn\\\\'t|shouldn\\\\'t|wouldn\\\\'t|couldn\\\\'t|can\\\\'t\\\\\\n|cannot|neither|nor)?\\\\s(able)\\\\b\")\n",
      "re.compile(\"(not|never|no|none|nobody|nothing|don\\\\'t\\\\\\n|doesn\\\\'t|won\\\\'t|shan\\\\'t|didn\\\\'t|shouldn\\\\'t|wouldn\\\\'t|couldn\\\\'t|can\\\\'t\\\\\\n|cannot|neither|nor)?\\\\s(abandon)\\\\b\")\n"
     ]
    }
   ],
   "source": [
    "# First, we update our function that compiles regular expressions\n",
    "def create_dict_regex_list_with_negators(dict_file:str):\n",
    "    \"\"\"Creates a list of regex expressions of dictionary terms.\"\"\"\n",
    "    with open (dict_file ,\"r\") as file:\n",
    "        # reads dictionary lines one -by -one\n",
    "        dict_terms = file.read().splitlines()\n",
    "        # the first capturing group in this Regex\n",
    "        # captures all possible negators , allowing for\n",
    "        # zero or one match as indicated by ? after the\n",
    "        # group ; the second group captures dictionary terms\n",
    "        dict_terms_regex = [re.compile(r\"(not|never|no|none|nobody|nothing|don\\'t\\\n",
    "|doesn\\'t|won\\'t|shan\\'t|didn\\'t|shouldn\\'t|wouldn\\'t|couldn\\'t|can\\'t\\\n",
    "|cannot|neither|nor)?\\s(\" + term + r\")\\b\") for term in dict_terms]\n",
    "        \n",
    "        # returns a list of Regex expressions that\n",
    "        # correspond to the input dictionary file ,\n",
    "        # allowing for negators\n",
    "        return dict_terms_regex\n",
    "    \n",
    "# Now we can apply our function to create Regex lists\n",
    "# for positive and negative dictionary terms\n",
    "positive_dict_regex = create_dict_regex_list_with_negators(positive_words_dict)\n",
    "negative_dict_regex = create_dict_regex_list_with_negators(negative_words_dict)  \n",
    "\n",
    "# prints the first entries of each Regex dictionary\n",
    "print(positive_dict_regex[0])\n",
    "print(negative_dict_regex[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the updated version of our function to calculate document tone is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'advantage')\n",
      "('', 'advantage')\n",
      "('', 'advantage')\n",
      "('', 'leading')\n",
      "('not', 'pleased')\n",
      "('', 'strong')\n",
      "('', 'valuable')\n",
      "(114, 6, 0, 5.2631578947368425)\n"
     ]
    }
   ],
   "source": [
    "# calculates tone with negators\n",
    "def get_tone2(input_text:str):\n",
    "    \"\"\"Counts All and Specific Words in Text, and checks for the presence of negators\"\"\"\n",
    "\n",
    "    # find all words in text\n",
    "    total_words = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", input_text)\n",
    "    total_words_count = len(total_words)\n",
    "    \n",
    "    # Positive Words #\n",
    "    # To account for negators , we can separately count\n",
    "    # positive and negated positive words\n",
    "    positive_word_count = 0\n",
    "    negated_positive_word_count = 0\n",
    "    \n",
    "    for regex in positive_dict_regex:\n",
    "        # searches for all occurences of Regex\n",
    "        matches = re.findall(regex, input_text)\n",
    "        for match in matches:\n",
    "            # if match is not empty\n",
    "            if len(match)>0:\n",
    "                # prints the match output ; this\n",
    "                # is for illustration purposes\n",
    "                # (i.e. , optional)\n",
    "                print(match)\n",
    "            # if the first element of the match\n",
    "            # is empty , no negator is present\n",
    "            if match[0] =='':\n",
    "                # so , increase the count of\n",
    "                # positive words by 1\n",
    "                positive_word_count += 1\n",
    "            else:\n",
    "                # otherwise , a negator is present ,\n",
    "                # so increase the count of negated\n",
    "                # positive words by 1\n",
    "                negated_positive_word_count += 1\n",
    "                \n",
    "    # If we are simply shifting the sentiment of negated\n",
    "    # positive words(from +1 to -1) , then the final\n",
    "    # positive word count is just :\n",
    "    positive_words_sum = positive_word_count\n",
    "    \n",
    "    # Repeat the same for Negative Words :\n",
    "    negative_word_count = 0\n",
    "    negated_negative_word_count = 0\n",
    "    \n",
    "    for regex in negative_dict_regex :\n",
    "        # search for all occurences of Regex\n",
    "        matches = re.findall(regex, input_text)\n",
    "        for match in matches :\n",
    "            # if match is not empty\n",
    "            if len(match)>0:\n",
    "                print(match)\n",
    "            # if the first element of the match\n",
    "            # is empty , no negator is present\n",
    "            if match[0] == '':\n",
    "                # so , increase the count of\n",
    "                # negative words by 1\n",
    "                negative_word_count += 1\n",
    "            else :\n",
    "                # otherwise , a negator is present , so\n",
    "                # increase the count of negated\n",
    "                # negative words by 1\n",
    "                negated_negative_word_count += 1\n",
    "    # If we are simply shifting the sentiment of negated\n",
    "    # negative words(from -1 to +1) , then the final\n",
    "    # negative word count is just :\n",
    "    negative_words_sum = negative_word_count\n",
    "            \n",
    "    # Then, Tone is:\n",
    "    tone = 100 *(positive_words_sum - negative_words_sum)/ total_words_count\n",
    "    return(total_words_count, positive_words_sum, negative_words_sum, tone)\n",
    "\n",
    "# Applying function get_tone2 function to an\n",
    "# example text :\n",
    "\n",
    "counts = get_tone2(\"\"\"At FedEx Ground , we have the market\n",
    "leading e-commerce portfolio. We continue to see\n",
    "strong demand across all customer segments with our\n",
    "new seven-day service . We will increase our speed\n",
    "advantage during the New Year. Our Sunday roll-out\n",
    "will speed up some lanes by one and two full\n",
    "transit days. This will increase our advantage\n",
    "significantly. And as you know, we are already\n",
    "faster by at least one day when compared to UPS's\n",
    "ground service in 25% of lanes. It is also really\n",
    "important to note our speed advantage and seven-day\n",
    "service is also very valuable for the premium B2B\n",
    "sectors, including healthcare and perishables\n",
    "shippers. Now, turning to Q2, I'm not pleased with\n",
    "our financial results.\"\"\")\n",
    "\n",
    "# output results\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is also possible that there are additional words between a given negator and a dictionary term (e.g., “not very encouraging” or “never went well”). In this case, we can modify the regular expressions above (in the dict_terms_regex list) and allow them to match phrases (N-grams) that contain dictionary terms (as opposed to individual dictionary terms). In other words, we can modify the regular expressions to allow for extra word(s) between the negators and dictionary terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the first capturing group in this Regex\n",
    "# captures all possible negators, allowing for\n",
    "# zero or one match as indicated by ? after the\n",
    "# group ; the second group captures dictionary terms ;\n",
    "# the non-capturing group ,s(:?\\w+\\s){0,2}, matches\n",
    "# either none ,one ,or two words between a negator\n",
    "# and a dictionary term.\n",
    "\n",
    "dict_terms_regex = [re.compile(r\"(not|never|no|none|nobody|nothing|don\\'t\\\n",
    "|doesn\\'t|won\\'t|shan\\'t|didn\\'t|shouldn\\'t|wouldn\\'t|couldn\\'t|can\\'t\\\n",
    "|cannot|neither|nor)?\\s(:?\\w+\\s){0,2}(\" + term + r\")\\b\") for term in dict_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "14_deep_computer_vision_with_cnns.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0401482a18a94f22b95d5321bfa6f414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c08c78c0d484eed9638ad2b757ab584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2839afc6cb6d4a50b0bdad1fcb7f39d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eefd1a01ef1c46e09ffbd97ad25377cf",
       "IPY_MODEL_d142189db76a4681a22f38ae252e4ebc",
       "IPY_MODEL_d441368305704ab9a3bdbe762ab340a4"
      ],
      "layout": "IPY_MODEL_1c08c78c0d484eed9638ad2b757ab584"
     }
    },
    "54a90429726b4d848358cafae87ad893": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57cbb645792f45adbfab9b29aa708809": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f0660be3bf44dd48fd42cd52a507e32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b681dc2200ad4ee397a46602e8f4f654": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d142189db76a4681a22f38ae252e4ebc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54a90429726b4d848358cafae87ad893",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0401482a18a94f22b95d5321bfa6f414",
      "value": 5
     }
    },
    "d441368305704ab9a3bdbe762ab340a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8ef3c06db574e3f88dc9a8c0bcd22ab",
      "placeholder": "​",
      "style": "IPY_MODEL_8f0660be3bf44dd48fd42cd52a507e32",
      "value": " 5/5 [00:10&lt;00:00,  2.12s/ file]"
     }
    },
    "eefd1a01ef1c46e09ffbd97ad25377cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b681dc2200ad4ee397a46602e8f4f654",
      "placeholder": "​",
      "style": "IPY_MODEL_57cbb645792f45adbfab9b29aa708809",
      "value": "Dl Completed...: 100%"
     }
    },
    "f8ef3c06db574e3f88dc9a8c0bcd22ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
